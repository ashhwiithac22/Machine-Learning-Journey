# Machine Learning Exercises

- This repository contains implementations and experiments with various **Machine Learning algorithms and techniques**.
- The exercises are designed to help understand the concepts, build intuition, and apply ML methods to datasets.

---

## 📂 Contents
The repository covers the following topics:

1. **Regression**
   - Simple Linear Regression
   - Multiple Linear Regression
   - Polynomial Regression

2. **Cross Validation**
   - k-Fold Cross Validation
   - Stratified k-Fold Cross Validation

3. **Decision Trees**
   - Classification Trees
   - Pruning

4. **Naive Bayes**
   - Gaussian Naive Bayes
   - Multinomial Naive Bayes
   - Bernoulli Naive Bayes

5. **K-Nearest Neighbors (KNN)**
   - Distance metrics (Euclidean)
   - Hyperparameter tuning (k-value)
   - Elbow curve to find k value

6. **Bagging**
   - Bootstrap Aggregation
   - Bagging Classifier

7. **Boosting**
   - AdaBoost
   - Gradient Boosting
   - XGBoost

8. **Random Forest**
   - Random Forest Classifier

---

## ⚙️ Tech Stack
- **Language:** Python 🐍  
- **Libraries Used:**  
  - `numpy`  
  - `pandas`  
  - `matplotlib` / `seaborn`  
  - `scikit-learn`  
  - `xgboost` (for Boosting)

---

## 🚀 How to Run
1. Clone this repository:
  
       git clone https://github.com/your-username/ML-Exercises.git
       cd ML-Exercises

2.Install dependencies:

       pip install -r requirements.txt

3.Run the scripts (example for regression):

      python regression.py

---

## 📊 Results

- Visualizations (scatter plots, decision boundaries, feature importance, etc.)
- Accuracy, Precision, Recall, F1-Score, RMSE depending on the model type
- Comparisons between different algorithms

---

## 📖 Learning Outcomes

- Understand the working of core ML algorithms
- Learn how to preprocess data and evaluate models
- Gain experience with ensemble methods like Bagging, Boosting, and Random Forest
