# ğŸ§  Machine Learning Lab â€“ Exercises

This repository contains hands-on exercises for various **Machine Learning algorithms and techniques**.  
Each folder corresponds to a specific topic with code, datasets, and visualizations.

---

## ğŸ“Œ Contents

### 1. Regression
- Simple Linear Regression
- Multiple Linear Regression
- Polynomial Regression
- Visualization of regression fits
- Evaluation using RÂ² score, MSE, and residual plots  

ğŸ“‚ Folder: `Regression/`

---

### 2. Cross Validation
- K-Fold Cross Validation
- Comparison of model performance across folds
- Visualizations: RÂ² score vs K-folds  

ğŸ“‚ Folder: `Cross_Validation/`

---

### 3. Decision Trees
- Building Classification Trees
- Tree visualization
- Confusion Matrix and performance metrics
- Depth vs Accuracy analysis  

ğŸ“‚ Folder: `Decision Trees/`

---

### 4. Naive Bayes Classifier
- Gaussian Naive Bayes (for continuous features)
- Multinomial Naive Bayes (for text classification)
- Bernoulli Naive Bayes (for binary features)
- Word frequency analysis (Spam/Ham dataset)
- Feature correlation and model tuning  

ğŸ“‚ Folder: `Naive Bayes Classifier/`

---

### 5. K-Nearest Neighbors (KNN)
- Distance-based classification
- Effect of K value on accuracy
- Standardization and scaling
- Confusion matrix and F1 score analysis  

ğŸ“‚ Folder: `KNN/`

---

### 6. Bagging Classifier
- Ensemble method using multiple base estimators
- Example: Decision Trees with Bagging
- Comparison with single decision tree performance
- Variance reduction and stability analysis  

ğŸ“‚ Folder: `Bagging Classifier/`

---

### 7. Boosting
- AdaBoost Classifier
- Gradient Boosting Classifier
- XgBoost Classifier
- Visualization of error reduction
- Performance comparison with other classifiers  

ğŸ“‚ Folder: `Boosting/`

---

### 8. Random Forest
- Ensemble of Decision Trees
- Feature importance ranking
- Out-of-Bag (OOB) error estimation
- Handling overfitting with Random Forest  

ğŸ“‚ Folder: `Random Forest/`

---

## âš™ï¸ Tech Stack
- **Python**
- **Pandas, NumPy** â€“ Data handling
- **Matplotlib, Seaborn** â€“ Visualization
- **Scikit-learn** â€“ Machine Learning models
- **Jupyter Notebook / VS Code / PyCharm**

---

## ğŸ¯ Objective
This lab is designed to:
- Understand core machine learning algorithms
- Implement models from scratch and with scikit-learn
- Evaluate performance using different metrics
- Compare classical ML approaches

---

## ğŸš€ How to Run
1. Clone this repository:
   ```bash
   git clone https://github.com/your-username/Machine-Learning-Journey.git
   cd Machine-Learning-Journey
   ```

2.Install dependencies:
```bash
pip install -r requirements.txt
```

3.Navigate to the respective folder and run the Python scripts or notebooks.
